{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Activation' from 'c:\\\\Users\\\\dylan\\\\OneDrive\\\\Desktop\\\\Professional Dev\\\\Project\\\\33DylanSmith33.github.io\\\\NN\\\\src\\\\scratchNN\\\\Activation.py'>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my modules\n",
    "import importlib\n",
    "import mnist_data_loader \n",
    "importlib.reload(mnist_data_loader)\n",
    "from mnist_data_loader import load_training_data, load_testing_data, one_hot_encode_label\n",
    "\n",
    "import Network\n",
    "importlib.reload(Network)\n",
    "from Network import Network\n",
    "\n",
    "import Optimizer\n",
    "importlib.reload(Optimizer)\n",
    "\n",
    "import Layer\n",
    "importlib.reload(Layer)\n",
    "\n",
    "import Activation\n",
    "importlib.reload(Activation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data_files/train.csv'\n",
    "test_path = 'data_files/test.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to implement Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = Network(train_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420, 784)\n",
      "784 420\n",
      "(420, 10)\n",
      "420\n"
     ]
    }
   ],
   "source": [
    "print(NN.train_input.shape)\n",
    "print(NN.num_input, NN.num_train_observations)\n",
    "print(NN.train_y.shape)\n",
    "print(NN.num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.add(Layer.Dense(input_size=NN.num_input, output_size=128))\n",
    "NN.add(Activation.ReLU())\n",
    "NN.add(Layer.Dense(input_size=128, output_size=10))\n",
    "NN.add(Activation.SoftMax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'Layer.Dense'>\n",
      "(128,)\n",
      "(784, 128)\n",
      "<class 'Activation.ReLU'>\n",
      "<class 'Layer.Dense'>\n",
      "(10,)\n",
      "(128, 10)\n",
      "<class 'Activation.SoftMax'>\n"
     ]
    }
   ],
   "source": [
    "for layer in NN.layers:\n",
    "    print(type(layer))\n",
    "    if issubclass(type(layer), (Layer.Dense)):\n",
    "        print(layer.biases.shape)\n",
    "        print(layer.weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward pass:\n",
    "- (42000, 784) dot (784, 128) + (128,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = NN.forward_pass(NN.train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 10)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'Layer.Dense'>\n",
      "(128,)\n",
      "(784, 128)\n",
      "<class 'Activation.ReLU'>\n",
      "(420, 128)\n",
      "<class 'Layer.Dense'>\n",
      "(10,)\n",
      "(128, 10)\n",
      "<class 'Activation.SoftMax'>\n",
      "(420, 10)\n"
     ]
    }
   ],
   "source": [
    "for layer in NN.layers:\n",
    "    print(type(layer))\n",
    "    if issubclass(type(layer), (Layer.Dense)):\n",
    "        print(layer.biases.shape)\n",
    "        print(layer.weights.shape)\n",
    "    elif issubclass(type(layer), (Activation.ReLU)):\n",
    "        print(layer.input.shape)\n",
    "    elif issubclass(type(layer), (Activation.SoftMax)):\n",
    "        print(layer.input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SoftMax back step\n",
      "    grad_output shape:  (420, 10)\n",
      "    input shape:  (420, 10)\n",
      "<class 'Activation.SoftMax'>\n",
      "<class 'Layer.Dense'>\n",
      "  ReLU back step\n",
      "    grad_output shape:  (420, 128)\n",
      "    input shape:  (420, 128)\n",
      "<class 'Activation.ReLU'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dylan\\OneDrive\\Desktop\\Professional Dev\\Project\\33DylanSmith33.github.io\\NN\\src\\scratchNN\\Network.py:79\u001b[0m, in \u001b[0;36mNetwork.backward_pass\u001b[1;34m(self, output)\u001b[0m\n\u001b[0;32m     76\u001b[0m         grad_output[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mbackward_step(grad_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;66;03m# grad_output = (holder, grad_output[1], grad_output[2])\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 79\u001b[0m         grad_output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(layer))\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# return the gradient of the loss with respect to the input of the network\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dylan\\OneDrive\\Desktop\\Professional Dev\\Project\\33DylanSmith33.github.io\\NN\\src\\scratchNN\\Layer.py:90\u001b[0m, in \u001b[0;36mDense.backward_step\u001b[1;34m(self, grad_output)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;124;03m    Back propagate the gradient through the layer\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m    :param grad_output: The gradient of the loss w.r.t the output of the layer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    :return: The gradient of the loss w.r.t the input of the layer\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# calculates the gradient of the loss function w.r.t the layers weights\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# derived using the chain rule, combining the gradient of the loss w.r.t the output of the layer (grad_output) and the layers input (self.input)\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# transpose of input is necessary for mat mult to match dimensions\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# essentially shows how changes in weights would affect the loss\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m grad_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# calculates the gradient of the loss function w.r.t the layers biases\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# since biases are added directly to the weighted inputs (i.e. deriv is 1), their gradients are just the sum of the gradients passed to the layer summed across the batch\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# axis=0 sums across rows (batch dimension) meaning we are summing across the batch dimension\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;66;03m# effectively collapsing 'batch_size' number of gradients into a single gradient (num_batches -> 1)\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# keepdims=True ensures that the output has the same dimensions as the input \u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# summing gradients across all examples in the batch gives you a total gradient for each bias that reflects its influence on the loss across the entire batch\u001b[39;00m\n\u001b[0;32m     98\u001b[0m grad_biases \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(grad_output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "NN.backward_pass(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1130209  0.09624661 0.051509   0.12440499 0.19731321 0.09568634\n",
      "  0.07823322 0.16174878 0.03057368 0.05126327]\n",
      " [0.11363454 0.09538257 0.05155662 0.12385626 0.1993287  0.09563252\n",
      "  0.07803592 0.16009049 0.03039475 0.05208764]\n",
      " [0.11280268 0.09491886 0.05167692 0.12417588 0.19946257 0.09471417\n",
      "  0.07735425 0.16353513 0.03026359 0.05109595]\n",
      " [0.11256752 0.09517792 0.05142661 0.12456514 0.19790818 0.09650758\n",
      "  0.07781541 0.16230147 0.03009462 0.05163555]\n",
      " [0.1125579  0.09602904 0.0513156  0.1263202  0.19666518 0.09494167\n",
      "  0.07908617 0.16120651 0.03070348 0.05117425]]\n"
     ]
    }
   ],
   "source": [
    "print(output[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# add layers\n",
    "NN.add(Layer.Dense(input_size=NN.num_input, output_size=128))\n",
    "NN.add(Activation.ReLU())\n",
    "NN.add(Layer.Dense(input_size=128, output_size=10))\n",
    "NN.add(Activation.SoftMax())\n",
    "\n",
    "# set optimizer\n",
    "# have to get parameters from??\n",
    "# parameters come from Layer.get_parameters_and_gradients\n",
    "    # list of tuples of (parameters, gradients) -> weights and biases\n",
    "NN.compile(optimizer=Optimizer.SGD(parameters))\n",
    "\n",
    "# fit the model\n",
    "# NN.fit(train_x, train_y, epochs=10, batch_size=32)\n",
    "\n",
    "# Predict\n",
    "# have something to show the input test_x picture\n",
    "# predictions = NN.predict(test_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
